{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED_VALUE = 100\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "torch.cuda.manual_seed(SEED_VALUE)\n",
    "torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"./data/city_pollution_data.csv\")\n",
    "df = pd.read_csv(\"./data/US2019to2023.csv\")\n",
    "\n",
    "\n",
    "DROP_ONEHOT = True\n",
    "SEQ_LENGTH = 5\n",
    "\n",
    "\n",
    "if DROP_ONEHOT:\n",
    "  INPUT_DIM = 8\n",
    "else:\n",
    "  INPUT_DIM = 29\n",
    "\n",
    "HIDDEN_DIM = 32\n",
    "LAYER_DIM = 3\n",
    "\n",
    "\n",
    "normalization_type = 'mean_std' # 'max', mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_train_test_data(df):\n",
    "  # we'll mostly need median and variance values of features for most of our needs\n",
    "\n",
    "  for col in df.columns:\n",
    "    for x in [\"Country\", \"min\", \"max\", \"count\", \"County\", \"past_week\", \"latitude\", \"longitude\", \"State\", \"variance\"]:\n",
    "      if x in col:\n",
    "        df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "  # df[\"Population Staying at Home\"] = df[\"Population Staying at Home\"].apply(lambda x: x.replace(\",\", \"\"))\n",
    "  # df[\"Population Not Staying at Home\"] = df[\"Population Not Staying at Home\"].apply(lambda x: x.replace(\",\", \"\"))\n",
    "\n",
    "  # Now we want 2 more features. Which day of week it is and which month it is.\n",
    "  # Both of these will be one-hot and hence we'll add 7+12 = 19 more columns.\n",
    "  # Getting month id is easy from the datetime column. \n",
    "  # For day of week, we'll use datetime library.\n",
    "  \n",
    "  df['weekday'] = df['Date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").weekday())\n",
    "  df['month'] = df['Date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").month - 1)\n",
    "\n",
    "  # using one-hot on month and weekday\n",
    "  weekday_onehot = pd.get_dummies(df['weekday'])\n",
    "  weekday_onehot.columns = [\"day_\"+str(x) for x in weekday_onehot]\n",
    "  month_onehot = pd.get_dummies(df['month'])\n",
    "  month_onehot.columns = [\"month_\"+str(x) for x in month_onehot]\n",
    "\n",
    "  df.drop(['weekday', 'month'], axis=1, inplace=True)\n",
    "  df = df.join([weekday_onehot, month_onehot])\n",
    "\n",
    "  cities_list = list(set(df['City']))\n",
    "  print(cities_list)\n",
    "  cities_list.sort()\n",
    "  print(cities_list)\n",
    "  city_df = {}\n",
    "  test_indices_of_cities = {}\n",
    "  train_set = {}\n",
    "  test_set = {}\n",
    "  TEST_SET_SIZE = 60                                        \n",
    "\n",
    "  for city in cities_list:\n",
    "    city_df[city] = df[df['City'] == city].sort_values('Date').reset_index()\n",
    "    for col in city_df[city].columns:\n",
    "      if col in [\"median_pm25\", \"median_o3\", \"median_so2\", \"median_no2\", \"median_pm10\", \"median_co\"]:\n",
    "        continue\n",
    "      try:  \n",
    "        _mean = np.nanmean(city_df[city][col])\n",
    "        if np.isnan(_mean) == True:\n",
    "          _mean = 0\n",
    "        city_df[city][col] = city_df[city][col].fillna(_mean)\n",
    "      except:\n",
    "        pass\n",
    "    if city_df[city].shape[0] < 600 :\n",
    "      print(\"City with less than 600 data : {} {}\".format(city_df[city].shape[0], city))\n",
    "      del city_df[city]\n",
    "      continue\n",
    "    \n",
    "    test_index_start = random.randint(0, city_df[city].shape[0] - TEST_SET_SIZE)\n",
    "    test_indices_of_cities[city] = [test_index_start, test_index_start + TEST_SET_SIZE]\n",
    "\n",
    "    test_set[city] = city_df[city].iloc[test_index_start:test_index_start + TEST_SET_SIZE]\n",
    "    train_set[city] = city_df[city].drop(index=list(range(test_index_start, test_index_start + TEST_SET_SIZE)))\n",
    "\n",
    "  return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Queens', 'Little Rock', 'El Paso', 'Manhattan', 'Saint Paul', 'Salem', 'Austin', 'Las Vegas', 'Portland', 'Phoenix', 'Miami', 'Springfield', 'Staten Island', 'Richmond', 'Raleigh', 'San Diego', 'Indianapolis', 'Albuquerque', 'Charlotte', 'Omaha', 'Boise', 'Seattle', 'The Bronx', 'Fresno', 'Nashville', 'Denver', 'San Jose', 'Chicago', 'Los Angeles', 'Madison', 'Dallas', 'Columbus', 'Sacramento', 'Washington D.C.', 'Boston', 'Jackson', 'Jacksonville', 'Memphis', 'Oklahoma City', 'San Francisco', 'Tucson', 'Providence', 'Detroit', 'Baltimore', 'Oakland', 'Philadelphia', 'Fort Worth', 'San Antonio', 'Columbia', 'Salt Lake City', 'Brooklyn', 'Honolulu', 'Atlanta', 'Houston', 'Milwaukee', 'Hartford', 'Tallahassee']\n",
      "['Albuquerque', 'Atlanta', 'Austin', 'Baltimore', 'Boise', 'Boston', 'Brooklyn', 'Charlotte', 'Chicago', 'Columbia', 'Columbus', 'Dallas', 'Denver', 'Detroit', 'El Paso', 'Fort Worth', 'Fresno', 'Hartford', 'Honolulu', 'Houston', 'Indianapolis', 'Jackson', 'Jacksonville', 'Las Vegas', 'Little Rock', 'Los Angeles', 'Madison', 'Manhattan', 'Memphis', 'Miami', 'Milwaukee', 'Nashville', 'Oakland', 'Oklahoma City', 'Omaha', 'Philadelphia', 'Phoenix', 'Portland', 'Providence', 'Queens', 'Raleigh', 'Richmond', 'Sacramento', 'Saint Paul', 'Salem', 'Salt Lake City', 'San Antonio', 'San Diego', 'San Francisco', 'San Jose', 'Seattle', 'Springfield', 'Staten Island', 'Tallahassee', 'The Bronx', 'Tucson', 'Washington D.C.']\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = get_train_test_data(df)\n",
    "\n",
    "cities_list = list(train_set.keys())\n",
    "\n",
    "all_train = pd.DataFrame()\n",
    "for city in cities_list:\n",
    "  all_train = all_train.append(train_set[city], ignore_index=True)\n",
    "\n",
    "all_test = pd.DataFrame({})\n",
    "for city in test_set:\n",
    "  all_test = all_test.append(test_set[city], ignore_index=True)\n",
    "\n",
    "concat_df = pd.concat([all_train,all_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_max = {}\n",
    "col_mean = {}\n",
    "col_mean2 = {}\n",
    "col_std = {}\n",
    "\n",
    "for city in cities_list:\n",
    "  col_mean[city] = {}\n",
    "  for col in train_set[city]:\n",
    "    if col in [\"index\", \"Date\", \"City\"]:\n",
    "      continue\n",
    "\n",
    "    train_set[city][col] = train_set[city][col].astype(\"float\")\n",
    "    test_set[city][col] = test_set[city][col].astype(\"float\")\n",
    "\n",
    "    if col in [\"median_pm25\", \"median_o3\", \"median_so2\", \"median_no2\", \"median_pm10\", \"median_co\"]:\n",
    "      _mean = np.nanmean(train_set[city][col])\n",
    "      if np.isnan(_mean) == True:\n",
    "        _mean = 0\n",
    "      \n",
    "      col_mean[city][col] = _mean\n",
    "      train_set[city][col] = train_set[city][col].fillna(_mean)\n",
    "\n",
    "    if normalization_type == 'mean_std':\n",
    "      col_mean2[col] = np.nanmean(concat_df[col].astype(\"float\"))\n",
    "      col_std[col] = np.nanstd(concat_df[col].astype(\"float\"))\n",
    "      train_set[city][col] = (train_set[city][col] - col_mean2[col]) / (col_std[col] + 0.001)\n",
    "      test_set[city][col] = (test_set[city][col] - col_mean2[col]) / (col_std[col] + 0.001)\n",
    "\n",
    "    else:\n",
    "      col_max[col] = concat_df[col].astype(\"float\").max()\n",
    "      train_set[city][col] = train_set[city][col] / (col_max[col] + 0.001)\n",
    "      test_set[city][col] = test_set[city][col] / (col_max[col] + 0.001)\n",
    "\n",
    "  if DROP_ONEHOT:\n",
    "    train_set[city].drop(train_set[city].columns[-19:], axis=1, inplace=True)\n",
    "    test_set[city].drop(test_set[city].columns[-19:], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of cities :  57\n",
      "Albuquerque(1613), Atlanta(1613), Austin(1613), Baltimore(1613), Boise(1613), Boston(1611), Brooklyn(1613), Charlotte(1613), Chicago(1613), Columbia(545), Columbus(1613), Dallas(1613), Denver(1612), Detroit(1545), El Paso(1613), Fort Worth(1610), Fresno(1613), Hartford(1613), Honolulu(1612), Houston(1613), Indianapolis(1593), Jackson(1613), Jacksonville(1613), Las Vegas(833), Little Rock(1613), Los Angeles(1613), Madison(1613), Manhattan(1613), Memphis(1613), Miami(1613), Milwaukee(1613), Nashville(1613), Oakland(1613), Oklahoma City(1613), Omaha(1613), Philadelphia(1613), Phoenix(1613), Portland(1613), Providence(1613), Queens(1613), Raleigh(1613), Richmond(1596), Sacramento(1613), Saint Paul(1613), Salem(1613), Salt Lake City(1613), San Antonio(1612), San Diego(1455), San Francisco(1613), San Jose(1613), Seattle(1613), Springfield(1605), Staten Island(1613), Tallahassee(1613), The Bronx(1613), Tucson(1583), Washington D.C.(1613), "
     ]
    }
   ],
   "source": [
    "# number of data per city\n",
    "\n",
    "print(\"num of cities : \", len(cities_list))\n",
    "for city in cities_list:\n",
    "    print(city+\"({})\".format(len(train_set[city])), end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'The Bronx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53    -0.188060\n",
       "54          NaN\n",
       "55    -0.193684\n",
       "56    -0.188060\n",
       "57    -0.182436\n",
       "58    -0.176812\n",
       "59    -0.047459\n",
       "60    -0.030587\n",
       "61    -0.013715\n",
       "62    -0.182436\n",
       "63    -0.047459\n",
       "64    -0.058707\n",
       "65    -0.030587\n",
       "66     0.008781\n",
       "67    -0.008091\n",
       "68    -0.075580\n",
       "69    -0.041835\n",
       "70    -0.188060\n",
       "71    -0.047459\n",
       "72    -0.058707\n",
       "73    -0.013715\n",
       "74    -0.081204\n",
       "75    -0.188060\n",
       "76    -0.176812\n",
       "77    -0.069956\n",
       "78    -0.030587\n",
       "79    -0.075580\n",
       "80    -0.064331\n",
       "81    -0.188060\n",
       "82    -0.182436\n",
       "83    -0.064331\n",
       "84    -0.064331\n",
       "85    -0.188060\n",
       "86    -0.081204\n",
       "87     0.065021\n",
       "88    -0.047459\n",
       "89    -0.188060\n",
       "90    -0.182436\n",
       "91    -0.081204\n",
       "92    -0.086828\n",
       "93    -0.081204\n",
       "94    -0.182436\n",
       "95    -0.047459\n",
       "96    -0.024963\n",
       "97    -0.036211\n",
       "98    -0.047459\n",
       "99    -0.188060\n",
       "100   -0.086828\n",
       "101   -0.069956\n",
       "102   -0.024963\n",
       "103   -0.064331\n",
       "104   -0.086828\n",
       "105   -0.086828\n",
       "106   -0.086828\n",
       "107   -0.092452\n",
       "108   -0.064331\n",
       "109   -0.103700\n",
       "110         NaN\n",
       "111   -0.041835\n",
       "112   -0.047459\n",
       "Name: median_co, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[city]['median_co']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = test_set[city]['median_co'][110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# df['City']['Portland']\n",
    "count = 0\n",
    "for co in test_set[city]['median_co']:\n",
    "    if np.isnan(co):\n",
    "        count += 1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
