{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import lime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED_VALUE = 100\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "torch.cuda.manual_seed(SEED_VALUE)\n",
    "torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# city pollution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/city_pollution_data.csv\")\n",
    "\n",
    "\n",
    "DROP_ONEHOT = True\n",
    "SEQ_LENGTH = 7\n",
    "\n",
    "\n",
    "if DROP_ONEHOT:\n",
    "  INPUT_DIM = 10 \n",
    "else:\n",
    "  INPUT_DIM = 29\n",
    "\n",
    "HIDDEN_DIM = 32\n",
    "LAYER_DIM = 3\n",
    "\n",
    "\n",
    "normalization_type = 'mean_std' # 'max', mean_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_train_test_data(df):\n",
    "  # we'll mostly need median and variance values of features for most of our needs\n",
    "\n",
    "  for col in df.columns:\n",
    "    for x in [\"min\", \"max\", \"count\", \"County\", \"past_week\", \"latitude\", \"longitude\", \"State\", \"variance\"]:\n",
    "      if x in col:\n",
    "        df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "  df[\"Population Staying at Home\"] = df[\"Population Staying at Home\"].apply(lambda x: x.replace(\",\", \"\"))\n",
    "  df[\"Population Not Staying at Home\"] = df[\"Population Not Staying at Home\"].apply(lambda x: x.replace(\",\", \"\"))\n",
    "\n",
    "  # Now we want 2 more features. Which day of week it is and which month it is.\n",
    "  # Both of these will be one-hot and hence we'll add 7+12 = 19 more columns.\n",
    "  # Getting month id is easy from the datetime column. \n",
    "  # For day of week, we'll use datetime library.\n",
    "  \n",
    "  df['weekday'] = df['Date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").weekday())\n",
    "  df['month'] = df['Date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").month - 1)\n",
    "\n",
    "  # using one-hot on month and weekday\n",
    "  weekday_onehot = pd.get_dummies(df['weekday'])\n",
    "  weekday_onehot.columns = [\"day_\"+str(x) for x in weekday_onehot]\n",
    "  month_onehot = pd.get_dummies(df['month'])\n",
    "  month_onehot.columns = [\"month_\"+str(x) for x in month_onehot]\n",
    "\n",
    "  df.drop(['weekday', 'month'], axis=1, inplace=True)\n",
    "  df = df.join([weekday_onehot, month_onehot])\n",
    "\n",
    "  cities_list = list(set(df['City']))\n",
    "  city_df = {}\n",
    "  test_indices_of_cities = {}\n",
    "  train_set = {}\n",
    "  test_set = {}\n",
    "  TEST_SET_SIZE = 60                                        \n",
    "\n",
    "  for city in cities_list:\n",
    "    city_df[city] = df[df['City'] == city].sort_values('Date').reset_index()\n",
    "    for col in city_df[city].columns:\n",
    "      if col in [\"pm25_median\", \"o3_median\", \"so2_median\", \"no2_median\", \"pm10_median\", \"co_median\"]:\n",
    "        continue\n",
    "      try:  \n",
    "        _mean = np.nanmean(city_df[city][col])\n",
    "        if np.isnan(_mean) == True:\n",
    "          _mean = 0\n",
    "        city_df[city][col] = city_df[city][col].fillna(_mean)\n",
    "      except:\n",
    "        pass\n",
    "    \n",
    "    test_index_start = random.randint(0, city_df[city].shape[0] - TEST_SET_SIZE)\n",
    "    test_indices_of_cities[city] = [test_index_start, test_index_start + TEST_SET_SIZE]\n",
    "\n",
    "    test_set[city] = city_df[city].iloc[test_index_start:test_index_start + TEST_SET_SIZE]\n",
    "    train_set[city] = city_df[city].drop(index=list(range(test_index_start, test_index_start + TEST_SET_SIZE)))\n",
    "\n",
    "  return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = get_train_test_data(df)\n",
    "\n",
    "cities_list = list(train_set.keys())\n",
    "\n",
    "all_train = pd.DataFrame()\n",
    "for city in cities_list:\n",
    "  all_train = all_train.append(train_set[city], ignore_index=True)\n",
    "\n",
    "all_test = pd.DataFrame({})\n",
    "for city in test_set:\n",
    "  all_test = all_test.append(test_set[city], ignore_index=True)\n",
    "\n",
    "concat_df = pd.concat([all_train,all_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_max = {}\n",
    "col_mean = {}\n",
    "col_mean2 = {}\n",
    "col_std = {}\n",
    "\n",
    "for city in cities_list:\n",
    "  col_mean[city] = {}\n",
    "  for col in train_set[city]:\n",
    "    if col in [\"index\", \"Date\", \"City\"]:\n",
    "      continue\n",
    "\n",
    "    train_set[city][col] = train_set[city][col].astype(\"float\")\n",
    "    test_set[city][col] = test_set[city][col].astype(\"float\")\n",
    "\n",
    "    if col in [\"pm25_median\", \"o3_median\", \"so2_median\", \"no2_median\", \"pm10_median\", \"co_median\"]:\n",
    "      _mean = np.nanmean(train_set[city][col])\n",
    "      if np.isnan(_mean) == True:\n",
    "        _mean = 0\n",
    "      \n",
    "      col_mean[city][col] = _mean\n",
    "      train_set[city][col] = train_set[city][col].fillna(_mean)\n",
    "\n",
    "    if normalization_type == 'mean_std':\n",
    "      col_mean2[col] = np.nanmean(concat_df[col].astype(\"float\"))\n",
    "      col_std[col] = np.nanstd(concat_df[col].astype(\"float\"))\n",
    "      train_set[city][col] = (train_set[city][col] - col_mean2[col]) / (col_std[col] + 0.001)\n",
    "      test_set[city][col] = (test_set[city][col] - col_mean2[col]) / (col_std[col] + 0.001)\n",
    "\n",
    "    else:\n",
    "      col_max[col] = concat_df[col].astype(\"float\").max()\n",
    "      train_set[city][col] = train_set[city][col] / (col_max[col] + 0.001)\n",
    "      test_set[city][col] = test_set[city][col] / (col_max[col] + 0.001)\n",
    "\n",
    "  if DROP_ONEHOT:\n",
    "    train_set[city].drop(train_set[city].columns[-19:], axis=1, inplace=True)\n",
    "    test_set[city].drop(test_set[city].columns[-19:], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of cities :  54\n",
      "tallahassee(608), brooklyn(607), jackson(607), las vegas(603), san antonio(605), oakland(608), san jose(608), denver(608), detroit(608), honolulu(608), jacksonville(608), omaha(608), columbia(490), sacramento(608), oklahoma city(608), austin(606), san francisco(608), charlotte(607), memphis(607), houston(607), little rock(608), madison(606), portland(607), tucson(608), miami(608), chicago(608), philadelphia(607), salt lake city(607), springfield(600), manhattan(608), boston(606), milwaukee(607), nashville(606), baltimore(608), indianapolis(599), los angeles(608), boise(608), staten island(607), columbus(607), saint paul(608), albuquerque(608), richmond(589), dallas(606), fresno(608), san diego(324), phoenix(608), providence(607), fort worth(603), atlanta(608), salem(608), seattle(606), hartford(608), raleigh(608), el paso(607), "
     ]
    }
   ],
   "source": [
    "# number of data per city\n",
    "\n",
    "print(\"num of cities : \", len(cities_list))\n",
    "for city in cities_list:\n",
    "    print(city+\"({})\".format(len(train_set[city])), end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sequence data\n",
    "class CityDataP(torch.utils.data.Dataset):\n",
    "  def __init__(self, selected_column, split):\n",
    "    self.split = split\n",
    "    if split == \"train\":\n",
    "      self.dataset = train_set\n",
    "    else:\n",
    "      self.dataset = test_set\n",
    "\n",
    "    self.valid_city_idx = 0\n",
    "    self.valid_day_idx = 0\n",
    "    self.selected_column = selected_column\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if self.split != \"train\":\n",
    "      # getting all data out of the validation set\n",
    "      out, city = self.get_idx_data(idx)\n",
    "      # print(out.columns)\n",
    "    \n",
    "    else:\n",
    "      # getting data randomly for train split\n",
    "      city = random.choice(cities_list)\n",
    "      _df = self.dataset[city]\n",
    "      start_idx = random.randint(1,_df.shape[0]-SEQ_LENGTH)\n",
    "      out =  _df.iloc[start_idx-1:start_idx+SEQ_LENGTH]\n",
    "\n",
    "    out = out.drop(['index', 'Date', 'City'], axis=1)\n",
    "\n",
    "    Y = pd.DataFrame({})\n",
    "    Y_true = pd.DataFrame({})\n",
    "\n",
    "    for col in out.columns:\n",
    "      if col == self.selected_column:\n",
    "        Y_true[col] = out[col]\n",
    "        Y[col] = out[col].fillna(col_mean[city][col])\n",
    "      \n",
    "      if col in [\"pm25_median\", \"pm10_median\", \"o3_median\", \"so2_median\", \"no2_median\", \"co_median\"]:\n",
    "        out.drop([col], axis=1, inplace=True)\n",
    "      else:\n",
    "        out[col] = out[col].astype(\"float\")\n",
    "    # print(out.columns) \n",
    "    out = np.concatenate((np.array(out)[1:,:], np.array(Y)[:-1,:]), axis=1)\n",
    "    Y = np.array(Y)[1:]\n",
    "    Y_true = np.array(Y_true)[1:]\n",
    "    \n",
    "    return out, Y, Y_true\n",
    "\n",
    "  def get_idx_data(self, idx):\n",
    "    city_idx = self.valid_city_idx % len(cities_list)\n",
    "    # city = cities_list[self.valid_city_idx]\n",
    "    city = cities_list[city_idx]\n",
    "    _df = self.dataset[city]\n",
    "\n",
    "    out =  _df.iloc[self.valid_day_idx:self.valid_day_idx+SEQ_LENGTH]\n",
    "    \n",
    "    if self.valid_day_idx+SEQ_LENGTH >= _df.shape[0]:\n",
    "      self.valid_day_idx = 0\n",
    "      self.valid_city_idx += 1\n",
    "    else:\n",
    "      self.valid_day_idx += 1\n",
    "\n",
    "    return out, city\n",
    "\n",
    "  def __len__(self):\n",
    "    if self.split != \"train\":\n",
    "      return (61-SEQ_LENGTH)*len(cities_list)\n",
    "    return len(all_train) - (SEQ_LENGTH - 1)*len(cities_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityDataForecast(torch.utils.data.Dataset):\n",
    "  def __init__(self, selected_column, split):\n",
    "    self.split = split\n",
    "    if split == \"train\":\n",
    "      self.dataset = train_set\n",
    "    else:\n",
    "      self.dataset = test_set\n",
    "\n",
    "    self.valid_city_idx = 0\n",
    "    self.valid_day_idx = 0\n",
    "    self.selected_column = selected_column\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    city = random.choice(cities_list)\n",
    "    _df = self.dataset[city]\n",
    "    start_idx = random.randint(1,_df.shape[0]-1)\n",
    "    out =  _df.iloc[start_idx-1:start_idx+1]\n",
    "\n",
    "    out = out.drop(['index', 'Date', 'City'], axis=1)\n",
    "\n",
    "    Y = pd.DataFrame({})\n",
    "    Y_true = pd.DataFrame({})\n",
    "\n",
    "    for col in out.columns:\n",
    "      if col == self.selected_column:\n",
    "        Y_true[col] = out[col]\n",
    "        #print(out[col])\n",
    "        Y[col] = out[col].fillna(col_mean[city][col])\n",
    "      \n",
    "      if col in [\"pm25_median\", \"pm10_median\", \"o3_median\", \"so2_median\", \"no2_median\", \"co_median\"]:\n",
    "        out.drop([col], axis=1, inplace=True)\n",
    "      else:\n",
    "        out[col] = out[col].astype(\"float\")\n",
    "\n",
    "    # print(out.shape)\n",
    "    out = np.concatenate((np.array(out)[1:,:], np.array(Y)[:-1,:]), axis=1)\n",
    "    Y = np.array(Y)[1:]\n",
    "    Y_true = np.array(Y_true)[1:]\n",
    "    \n",
    "\n",
    "    return out, Y, Y_true\n",
    "\n",
    "  def get_idx_data(self, idx):\n",
    "    city = cities_list[self.valid_city_idx]\n",
    "    _df = self.dataset[city]\n",
    "\n",
    "    out =  _df.iloc[self.valid_day_idx:self.valid_day_idx+1]\n",
    "    \n",
    "    if self.valid_day_idx+1 >= _df.shape[0]:\n",
    "      self.valid_day_idx = 0\n",
    "      self.valid_city_idx += 1\n",
    "    else:\n",
    "      self.valid_day_idx += 1\n",
    "\n",
    "    return out, city\n",
    "\n",
    "  def __len__(self):\n",
    "    if self.split != \"train\":\n",
    "      return (61-1)*len(cities_list)\n",
    "    return len(all_train) - (1 - 1)*len(cities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that implement the look_ahead mask for masking future time steps. \n",
    "def create_look_ahead_mask(size, device):\n",
    "    mask = torch.ones((size, size), device=device)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    return mask  # (size, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7df9e6e2d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance of each linear & nonlinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearning.models import *\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluation(val_loader, model, model_name, LUR, SELECTED_COLUMN, mask=False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mse_list = []\n",
    "    total_se = 0.0\n",
    "    total_pe = 0.0\n",
    "    total_valid = 0.0\n",
    "\n",
    "    for x_val, _, y_val in val_loader:\n",
    "        x_val, y_val = [t.cuda().float() for t in (x_val, y_val)]\n",
    "        \n",
    "        if mask:\n",
    "            masking = create_look_ahead_mask(x_val.shape[1], device)\n",
    "            out, _ = model(x_val.to(device), masking)\n",
    "        else:\n",
    "            out = model(x_val.to(device))\n",
    "\n",
    "        if LUR :\n",
    "            ytrue = y_val[:,-1,:].squeeze().cpu().numpy()\n",
    "            ypred = out[:,-1,:].squeeze().cpu().detach().numpy()\n",
    "        else:\n",
    "            ytrue = y_val[:,-1,:].squeeze().cpu().numpy()\n",
    "            ypred = out[:,-1,:].squeeze().cpu().detach().numpy()\n",
    "        true_valid = np.isnan(ytrue) != 1\n",
    "        ytrue = ytrue[true_valid] #np.nan_to_num(ytrue, 0)\n",
    "        ypred = ypred[true_valid]\n",
    "\n",
    "        if normalization_type == 'mean_std':\n",
    "            ytrue = (ytrue * col_std[SELECTED_COLUMN]) + col_mean2[SELECTED_COLUMN]\n",
    "            ypred = (ypred * col_std[SELECTED_COLUMN]) + col_mean2[SELECTED_COLUMN]\n",
    "        \n",
    "        else:\n",
    "            ytrue = (ytrue * col_max[SELECTED_COLUMN])\n",
    "            ypred = (ypred * col_max[SELECTED_COLUMN])\n",
    "\n",
    "        se = (ytrue - ypred)**2 # np.square(ytrue - ypred)\n",
    "        pe = np.abs((ytrue - ypred) / (ytrue + 0.0001))\n",
    "        total_se += np.sum(se)\n",
    "        total_pe += np.sum(pe)\n",
    "        total_valid += np.sum(true_valid)\n",
    "\n",
    "    eval_mse = total_se / total_valid # np.mean(se) # \n",
    "    eval_mape = total_pe / total_valid # np.mean(pe) # \n",
    "    print('valid samples:', total_valid)\n",
    "    print('Eval MSE: ', eval_mse)\n",
    "    print('Eval RMSE: {}: '.format(SELECTED_COLUMN), np.sqrt(eval_mse))\n",
    "    print('Eval MAPE: {}: '.format(SELECTED_COLUMN), eval_mape*100)\n",
    "    \n",
    "    return eval_mse, eval_mape*100\n",
    "\n",
    "\n",
    "# Train\n",
    "def train(sampleLoader, val_loader, model, model_name, SELECTED_COLUMN, mask=False, LUR=False, l1=False, l2=False):\n",
    "\n",
    "    lr = 0.001\n",
    "    n_epochs = 10   \n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # LUR\n",
    "    if LUR:\n",
    "        print(\"set l1,l2 loss\")\n",
    "        l1_lmbda = 0.01\n",
    "        l1_lmbda = torch.FloatTensor([l1_lmbda]).cuda()\n",
    "        l1_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "        l2_lmbda = 0.01\n",
    "        l2_lmbda = torch.FloatTensor([l2_lmbda]).cuda()\n",
    "        l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "    # DL\n",
    "    else:\n",
    "        print(\"set SoftDTW loss\")\n",
    "        lmbda = 0.5\n",
    "        dtw_loss = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "      \n",
    "    print('Start ' + model_name + ' training')\n",
    "    best_mse = 2000.0\n",
    "    mape = 2000.0\n",
    "    best_model = None\n",
    "    \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        batch_idx = 0\n",
    "        bar = tqdm(sampleLoader)\n",
    "\n",
    "        model.train()\n",
    "        for x_batch, y_batch, _ in bar:\n",
    "            model.train()\n",
    "            x_batch = x_batch.cuda().float()\n",
    "            y_batch = y_batch.cuda().float()\n",
    "\n",
    "            \n",
    "            if mask==True:\n",
    "                masking = create_look_ahead_mask(x_batch.shape[1], device)\n",
    "                out, _ = model(x_batch.to(device), masking)\n",
    "            else :\n",
    "                out = model(x_batch.to(device))\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            if LUR:\n",
    "                # LASSO\n",
    "                if l1==True and l2==False:\n",
    "                    l1_reg = torch.norm(model.linear.weight, p=1)\n",
    "                    loss = criterion(out[:,-1,:], y_batch[:,-1,:]) + l1_lmbda * l1_reg\n",
    "                # Ridge\n",
    "                elif l1==False and l2==True:\n",
    "                    l2_reg = torch.norm(model.linear.weight, p=2)\n",
    "                    loss = criterion(out[:,-1,:], y_batch[:,-1,:]) + l2_lmbda * l2_reg\n",
    "                # Elastic\n",
    "                elif l1==True and l2==True:\n",
    "                    l1_reg = torch.norm(model.linear.weight, p=1)\n",
    "                    l2_reg = torch.norm(model.linear.weight, p=2)\n",
    "                    loss = criterion(out[:,-1,:], y_batch[:,-1,:]) + l1_lmbda * l1_reg + l2_lmbda * l2_reg\n",
    "                # OLS\n",
    "                else:\n",
    "                    loss = criterion(out[:,-1,:], y_batch[:,-1,:])\n",
    "            else:\n",
    "                loss = criterion(out[:,-1,:], y_batch[:,-1,:]) + lmbda * dtw_loss(out.cuda(),y_batch.cuda()).mean()\n",
    "\n",
    "            epoch_loss = (epoch_loss*batch_idx + loss.item())/(batch_idx+1)\n",
    "            loss.backward(retain_graph=True)\n",
    "            opt.step()\n",
    "\n",
    "            bar.set_description(str(epoch_loss))\n",
    "            batch_idx += 1\n",
    "\n",
    "        eval_mse, eval_mape = evaluation(val_loader, model, model_name, LUR, SELECTED_COLUMN, mask)\n",
    "        \n",
    "\n",
    "        if eval_mse < best_mse:\n",
    "            best_model = deepcopy(model)\n",
    "            best_mse = eval_mse\n",
    "            mape = eval_mape\n",
    "            torch.save(best_model.state_dict(), \"./DeepLearning/save/\"+SELECTED_COLUMN+\"/\"+model_name+\".pth\")\n",
    "      \n",
    "    print(model_name)   \n",
    "    print(\"Best MSE :\", best_mse)\n",
    "    print(\"RMSE :\", np.sqrt(best_mse))\n",
    "    print(\"MAPE :\", mape)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearning.models import *\n",
    "from DeepLearning.loss_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set l1,l2 loss\n",
      "Start OLS training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.8516845010930991:  86%|████████▌ | 868/1012 [04:00<00:44,  3.25it/s]"
     ]
    }
   ],
   "source": [
    "SELECTED_COLUMN = \"so2_median\" # [\"pm25_median\", \"so2_median\", \"pm10_median\", \"no2_median\", \"o3_median\", \"co_median\"]:\n",
    "train_data = CityDataForecast(SELECTED_COLUMN, \"train\")\n",
    "val_data = CityDataForecast(SELECTED_COLUMN, \"test\")\n",
    "sampleLoader = DataLoader(train_data, 32, shuffle=True, num_workers=4, worker_init_fn=seed_worker, generator=g)\n",
    "val_loader = DataLoader(val_data, 4096, shuffle=False, num_workers=4, worker_init_fn=seed_worker, generator=g)\n",
    "OLS = LinearRegression(input_dim=11)\n",
    "train(sampleLoader, val_loader, OLS, \"OLS\", SELECTED_COLUMN, LUR=True)\n",
    "LASSO = LinearRegression(input_dim=11)\n",
    "train(sampleLoader, val_loader, LASSO, \"LASSO\", SELECTED_COLUMN, LUR=True, l1=True)\n",
    "Ridge = LinearRegression(input_dim=11)\n",
    "train(sampleLoader, val_loader, Ridge, \"Ridge\", SELECTED_COLUMN, LUR=True, l2=True)\n",
    "Elastic = LinearRegression(input_dim=11)\n",
    "train(sampleLoader, val_loader, Elastic, \"Elastic\", SELECTED_COLUMN, LUR=True, l1=True, l2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
