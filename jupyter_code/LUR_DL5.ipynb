{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import lime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED_VALUE = 100\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "torch.cuda.manual_seed(SEED_VALUE)\n",
    "torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# city pollution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/city_pollution_data.csv\")\n",
    "\n",
    "\n",
    "DROP_ONEHOT = True\n",
    "SEQ_LENGTH = 7\n",
    "\n",
    "\n",
    "if DROP_ONEHOT:\n",
    "  INPUT_DIM = 10 \n",
    "else:\n",
    "  INPUT_DIM = 29\n",
    "\n",
    "HIDDEN_DIM = 32\n",
    "LAYER_DIM = 3\n",
    "\n",
    "\n",
    "normalization_type = 'mean_std' # 'max', mean_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_train_test_data(df):\n",
    "  # we'll mostly need median and variance values of features for most of our needs\n",
    "\n",
    "  for col in df.columns:\n",
    "    for x in [\"min\", \"max\", \"count\", \"County\", \"past_week\", \"latitude\", \"longitude\", \"State\", \"variance\"]:\n",
    "      if x in col:\n",
    "        df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "  df[\"Population Staying at Home\"] = df[\"Population Staying at Home\"].apply(lambda x: x.replace(\",\", \"\"))\n",
    "  df[\"Population Not Staying at Home\"] = df[\"Population Not Staying at Home\"].apply(lambda x: x.replace(\",\", \"\"))\n",
    "\n",
    "  # Now we want 2 more features. Which day of week it is and which month it is.\n",
    "  # Both of these will be one-hot and hence we'll add 7+12 = 19 more columns.\n",
    "  # Getting month id is easy from the datetime column. \n",
    "  # For day of week, we'll use datetime library.\n",
    "  \n",
    "  df['weekday'] = df['Date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").weekday())\n",
    "  df['month'] = df['Date'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").month - 1)\n",
    "\n",
    "  # using one-hot on month and weekday\n",
    "  weekday_onehot = pd.get_dummies(df['weekday'])\n",
    "  weekday_onehot.columns = [\"day_\"+str(x) for x in weekday_onehot]\n",
    "  month_onehot = pd.get_dummies(df['month'])\n",
    "  month_onehot.columns = [\"month_\"+str(x) for x in month_onehot]\n",
    "\n",
    "  df.drop(['weekday', 'month'], axis=1, inplace=True)\n",
    "  df = df.join([weekday_onehot, month_onehot])\n",
    "\n",
    "  cities_list = list(set(df['City']))\n",
    "  city_df = {}\n",
    "  test_indices_of_cities = {}\n",
    "  train_set = {}\n",
    "  test_set = {}\n",
    "  TEST_SET_SIZE = 60                                        \n",
    "\n",
    "  for city in cities_list:\n",
    "    city_df[city] = df[df['City'] == city].sort_values('Date').reset_index()\n",
    "    for col in city_df[city].columns:\n",
    "      if col in [\"pm25_median\", \"o3_median\", \"so2_median\", \"no2_median\", \"pm10_median\", \"co_median\"]:\n",
    "        continue\n",
    "      try:  \n",
    "        _mean = np.nanmean(city_df[city][col])\n",
    "        if np.isnan(_mean) == True:\n",
    "          _mean = 0\n",
    "        city_df[city][col] = city_df[city][col].fillna(_mean)\n",
    "      except:\n",
    "        pass\n",
    "    \n",
    "    test_index_start = random.randint(0, city_df[city].shape[0] - TEST_SET_SIZE)\n",
    "    test_indices_of_cities[city] = [test_index_start, test_index_start + TEST_SET_SIZE]\n",
    "\n",
    "    test_set[city] = city_df[city].iloc[test_index_start:test_index_start + TEST_SET_SIZE]\n",
    "    train_set[city] = city_df[city].drop(index=list(range(test_index_start, test_index_start + TEST_SET_SIZE)))\n",
    "\n",
    "  return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = get_train_test_data(df)\n",
    "\n",
    "cities_list = list(train_set.keys())\n",
    "\n",
    "all_train = pd.DataFrame()\n",
    "for city in cities_list:\n",
    "  all_train = all_train.append(train_set[city], ignore_index=True)\n",
    "\n",
    "all_test = pd.DataFrame({})\n",
    "for city in test_set:\n",
    "  all_test = all_test.append(test_set[city], ignore_index=True)\n",
    "\n",
    "concat_df = pd.concat([all_train,all_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_max = {}\n",
    "col_mean = {}\n",
    "col_mean2 = {}\n",
    "col_std = {}\n",
    "\n",
    "for city in cities_list:\n",
    "  col_mean[city] = {}\n",
    "  for col in train_set[city]:\n",
    "    if col in [\"index\", \"Date\", \"City\"]:\n",
    "      continue\n",
    "\n",
    "    train_set[city][col] = train_set[city][col].astype(\"float\")\n",
    "    test_set[city][col] = test_set[city][col].astype(\"float\")\n",
    "\n",
    "    if col in [\"pm25_median\", \"o3_median\", \"so2_median\", \"no2_median\", \"pm10_median\", \"co_median\"]:\n",
    "      _mean = np.nanmean(train_set[city][col])\n",
    "      if np.isnan(_mean) == True:\n",
    "        _mean = 0\n",
    "      \n",
    "      col_mean[city][col] = _mean\n",
    "      train_set[city][col] = train_set[city][col].fillna(_mean)\n",
    "\n",
    "    if normalization_type == 'mean_std':\n",
    "      col_mean2[col] = np.nanmean(concat_df[col].astype(\"float\"))\n",
    "      col_std[col] = np.nanstd(concat_df[col].astype(\"float\"))\n",
    "      train_set[city][col] = (train_set[city][col] - col_mean2[col]) / (col_std[col] + 0.001)\n",
    "      test_set[city][col] = (test_set[city][col] - col_mean2[col]) / (col_std[col] + 0.001)\n",
    "\n",
    "    else:\n",
    "      col_max[col] = concat_df[col].astype(\"float\").max()\n",
    "      train_set[city][col] = train_set[city][col] / (col_max[col] + 0.001)\n",
    "      test_set[city][col] = test_set[city][col] / (col_max[col] + 0.001)\n",
    "\n",
    "  if DROP_ONEHOT:\n",
    "    train_set[city].drop(train_set[city].columns[-19:], axis=1, inplace=True)\n",
    "    test_set[city].drop(test_set[city].columns[-19:], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of cities :  54\n",
      "san jose(608), tallahassee(608), omaha(608), chicago(608), salem(608), miami(608), denver(608), providence(607), san francisco(608), seattle(606), fresno(608), honolulu(608), saint paul(608), indianapolis(599), springfield(600), columbia(490), austin(606), columbus(607), detroit(608), richmond(589), atlanta(608), hartford(608), houston(607), los angeles(608), sacramento(608), staten island(607), tucson(608), brooklyn(607), jacksonville(608), baltimore(608), san antonio(605), raleigh(608), portland(607), oakland(608), boise(608), memphis(607), charlotte(607), manhattan(608), salt lake city(607), albuquerque(608), el paso(607), philadelphia(607), jackson(607), nashville(606), madison(606), oklahoma city(608), san diego(324), las vegas(603), phoenix(608), little rock(608), fort worth(603), milwaukee(607), dallas(606), boston(606), "
     ]
    }
   ],
   "source": [
    "# number of data per city\n",
    "\n",
    "print(\"num of cities : \", len(cities_list))\n",
    "for city in cities_list:\n",
    "    print(city+\"({})\".format(len(train_set[city])), end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityDataP(torch.utils.data.Dataset):\n",
    "  def __init__(self, selected_column, split):\n",
    "    self.split = split\n",
    "    if split == \"train\":\n",
    "      self.dataset = train_set\n",
    "    else:\n",
    "      self.dataset = test_set\n",
    "\n",
    "    self.valid_city_idx = 0\n",
    "    self.valid_day_idx = 0\n",
    "    self.selected_column = selected_column\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if self.split != \"train\":\n",
    "      # getting all data out of the validation set\n",
    "      out, city = self.get_idx_data(idx)\n",
    "      # print(out.columns)\n",
    "    \n",
    "    else:\n",
    "      # getting data randomly for train split\n",
    "      city = random.choice(cities_list)\n",
    "      _df = self.dataset[city]\n",
    "      start_idx = random.randint(1,_df.shape[0]-SEQ_LENGTH)\n",
    "      out =  _df.iloc[start_idx-1:start_idx+SEQ_LENGTH]\n",
    "\n",
    "    out = out.drop(['index', 'Date', 'City'], axis=1)\n",
    "\n",
    "    Y = pd.DataFrame({})\n",
    "    Y_true = pd.DataFrame({})\n",
    "\n",
    "    for col in out.columns:\n",
    "      if col == self.selected_column:\n",
    "        Y_true[col] = out[col]\n",
    "        Y[col] = out[col].fillna(col_mean[city][col])\n",
    "      \n",
    "      if col in [\"pm25_median\", \"pm10_median\", \"o3_median\", \"so2_median\", \"no2_median\", \"co_median\"]:\n",
    "        out.drop([col], axis=1, inplace=True)\n",
    "      else:\n",
    "        out[col] = out[col].astype(\"float\")\n",
    "    # print(out.columns) \n",
    "    out = np.concatenate((np.array(out)[1:,:], np.array(Y)[:-1,:]), axis=1)\n",
    "    Y = np.array(Y)[1:]\n",
    "    Y_true = np.array(Y_true)[1:]\n",
    "    \n",
    "    return out, Y, Y_true\n",
    "\n",
    "  def get_idx_data(self, idx):\n",
    "    city_idx = self.valid_city_idx % len(cities_list)\n",
    "    # city = cities_list[self.valid_city_idx]\n",
    "    city = cities_list[city_idx]\n",
    "    _df = self.dataset[city]\n",
    "\n",
    "    out =  _df.iloc[self.valid_day_idx:self.valid_day_idx+SEQ_LENGTH]\n",
    "    \n",
    "    if self.valid_day_idx+SEQ_LENGTH >= _df.shape[0]:\n",
    "      self.valid_day_idx = 0\n",
    "      self.valid_city_idx += 1\n",
    "    else:\n",
    "      self.valid_day_idx += 1\n",
    "\n",
    "    return out, city\n",
    "\n",
    "  def __len__(self):\n",
    "    if self.split != \"train\":\n",
    "      return (61-SEQ_LENGTH)*len(cities_list)\n",
    "    return len(all_train) - (SEQ_LENGTH - 1)*len(cities_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityDataForecast(torch.utils.data.Dataset):\n",
    "  def __init__(self, selected_column, split):\n",
    "    self.split = split\n",
    "    if split == \"train\":\n",
    "      self.dataset = train_set\n",
    "    else:\n",
    "      self.dataset = test_set\n",
    "\n",
    "    self.valid_city_idx = 0\n",
    "    self.valid_day_idx = 0\n",
    "    self.selected_column = selected_column\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    city = random.choice(cities_list)\n",
    "    _df = self.dataset[city]\n",
    "    start_idx = random.randint(1,_df.shape[0]-1)\n",
    "    out =  _df.iloc[start_idx-1:start_idx+1]\n",
    "\n",
    "    out = out.drop(['index', 'Date', 'City'], axis=1)\n",
    "\n",
    "    Y = pd.DataFrame({})\n",
    "    Y_true = pd.DataFrame({})\n",
    "\n",
    "    for col in out.columns:\n",
    "      if col == self.selected_column:\n",
    "        Y_true[col] = out[col]\n",
    "        #print(out[col])\n",
    "        Y[col] = out[col].fillna(col_mean[city][col])\n",
    "      \n",
    "      if col in [\"pm25_median\", \"pm10_median\", \"o3_median\", \"so2_median\", \"no2_median\", \"co_median\"]:\n",
    "        out.drop([col], axis=1, inplace=True)\n",
    "      else:\n",
    "        out[col] = out[col].astype(\"float\")\n",
    "\n",
    "    # print(out.shape)\n",
    "    out = np.concatenate((np.array(out)[1:,:], np.array(Y)[:-1,:]), axis=1)\n",
    "    Y = np.array(Y)[1:]\n",
    "    Y_true = np.array(Y_true)[1:]\n",
    "    \n",
    "\n",
    "    return out, Y, Y_true\n",
    "\n",
    "  def get_idx_data(self, idx):\n",
    "    city = cities_list[self.valid_city_idx]\n",
    "    _df = self.dataset[city]\n",
    "\n",
    "    out =  _df.iloc[self.valid_day_idx:self.valid_day_idx+1]\n",
    "    \n",
    "    if self.valid_day_idx+1 >= _df.shape[0]:\n",
    "      self.valid_day_idx = 0\n",
    "      self.valid_city_idx += 1\n",
    "    else:\n",
    "      self.valid_day_idx += 1\n",
    "\n",
    "    return out, city\n",
    "\n",
    "  def __len__(self):\n",
    "    if self.split != \"train\":\n",
    "      return (61-1)*len(cities_list)\n",
    "    return len(all_train) - (1 - 1)*len(cities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that implement the look_ahead mask for masking future time steps. \n",
    "def create_look_ahead_mask(size, device):\n",
    "    mask = torch.ones((size, size), device=device)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    return mask  # (size, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1c46f69710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance of each linear & nonlinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearning.models import *\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluation(val_loader, model, model_name, LUR, SELECTED_COLUMN, mask=False):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mse_list = []\n",
    "    total_se = 0.0\n",
    "    total_pe = 0.0\n",
    "    total_valid = 0.0\n",
    "\n",
    "    for x_val, _, y_val in val_loader:\n",
    "        x_val, y_val = [t.cuda().float() for t in (x_val, y_val)]\n",
    "        \n",
    "        if mask:\n",
    "            masking = create_look_ahead_mask(x_val.shape[1], device)\n",
    "            out, _ = model(x_val.to(device), masking)\n",
    "        else:\n",
    "            out = model(x_val.to(device))\n",
    "\n",
    "        if LUR :\n",
    "            ytrue = y_val[:,-1,:].squeeze().cpu().numpy()\n",
    "            ypred = out[:,-1,:].squeeze().cpu().detach().numpy()\n",
    "        else:\n",
    "            ytrue = y_val[:,-1,:].squeeze().cpu().numpy()\n",
    "            ypred = out[:,-1,:].squeeze().cpu().detach().numpy()\n",
    "        true_valid = np.isnan(ytrue) != 1\n",
    "        ytrue = ytrue[true_valid] #np.nan_to_num(ytrue, 0)\n",
    "        ypred = ypred[true_valid]\n",
    "\n",
    "        if normalization_type == 'mean_std':\n",
    "            ytrue = (ytrue * col_std[SELECTED_COLUMN]) + col_mean2[SELECTED_COLUMN]\n",
    "            ypred = (ypred * col_std[SELECTED_COLUMN]) + col_mean2[SELECTED_COLUMN]\n",
    "        \n",
    "        else:\n",
    "            ytrue = (ytrue * col_max[SELECTED_COLUMN])\n",
    "            ypred = (ypred * col_max[SELECTED_COLUMN])\n",
    "\n",
    "        se = (ytrue - ypred)**2 # np.square(ytrue - ypred)\n",
    "        pe = np.abs((ytrue - ypred) / (ytrue + 0.0001))\n",
    "        total_se += np.sum(se)\n",
    "        total_pe += np.sum(pe)\n",
    "        total_valid += np.sum(true_valid)\n",
    "\n",
    "    eval_mse = total_se / total_valid # np.mean(se) # \n",
    "    eval_mape = total_pe / total_valid # np.mean(pe) # \n",
    "    print('valid samples:', total_valid)\n",
    "    print('Eval MSE: ', eval_mse)\n",
    "    print('Eval RMSE: {}: '.format(SELECTED_COLUMN), np.sqrt(eval_mse))\n",
    "    print('Eval MAPE: {}: '.format(SELECTED_COLUMN), eval_mape*100)\n",
    "    \n",
    "    return eval_mse, eval_mape*100\n",
    "\n",
    "\n",
    "# Train\n",
    "def train(sampleLoader, val_loader, model, model_name, SELECTED_COLUMN, mask=False, LUR=False, l1=False, l2=False):\n",
    "\n",
    "    lr = 0.001\n",
    "    n_epochs = 10   \n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # LUR\n",
    "    if LUR:\n",
    "        print(\"set l1,l2 loss\")\n",
    "        l1_lmbda = 0.01\n",
    "        l1_lmbda = torch.FloatTensor([l1_lmbda]).cuda()\n",
    "        l1_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "        l2_lmbda = 0.01\n",
    "        l2_lmbda = torch.FloatTensor([l2_lmbda]).cuda()\n",
    "        l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "    # DL\n",
    "    else:\n",
    "        print(\"set SoftDTW loss\")\n",
    "        lmbda = 0.5\n",
    "        dtw_loss = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "      \n",
    "    print('Start ' + model_name + ' training')\n",
    "    best_mse = 2000.0\n",
    "    mape = 2000.0\n",
    "    best_model = None\n",
    "    \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        batch_idx = 0\n",
    "        bar = tqdm(sampleLoader)\n",
    "\n",
    "        model.train()\n",
    "        for x_batch, y_batch, _ in bar:\n",
    "            model.train()\n",
    "            x_batch = x_batch.cuda().float()\n",
    "            y_batch = y_batch.cuda().float()\n",
    "\n",
    "            \n",
    "            if mask==True:\n",
    "                masking = create_look_ahead_mask(x_batch.shape[1], device)\n",
    "                out, _ = model(x_batch.to(device), masking)\n",
    "            else :\n",
    "                out = model(x_batch.to(device))\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            if LUR:\n",
    "                # LASSO\n",
    "                if l1==True and l2==False:\n",
    "                    l1_reg = torch.norm(model.linear.weight, p=1)\n",
    "                    loss = criterion(out[:,-1,:], y_batch[:,-1,:]) + l1_lmbda * l1_reg\n",
    "                # Ridge\n",
    "                elif l1==False and l2==True:\n",
    "                    l2_reg = torch.norm(model.linear.weight, p=2)\n",
    "                    loss = criterion(out[:,-1,:], y_batch[:,-1,:]) + l2_lmbda * l2_reg\n",
    "                # Elastic\n",
    "                elif l1==True and l2==True:\n",
    "                    l1_reg = torch.norm(model.linear.weight, p=1)\n",
    "                    l2_reg = torch.norm(model.linear.weight, p=2)\n",
    "                    loss = criterion(out[:,-1,:], y_batch[:,-1,:]) + l1_lmbda * l1_reg + l2_lmbda * l2_reg\n",
    "                # OLS\n",
    "                else:\n",
    "                    loss = criterion(out[:,-1,:], y_batch[:,-1,:])\n",
    "            else:\n",
    "                loss = criterion(out[:,-1,:], y_batch[:,-1,:]) + lmbda * dtw_loss(out.cuda(),y_batch.cuda()).mean()\n",
    "\n",
    "            epoch_loss = (epoch_loss*batch_idx + loss.item())/(batch_idx+1)\n",
    "            loss.backward(retain_graph=True)\n",
    "            opt.step()\n",
    "\n",
    "            bar.set_description(str(epoch_loss))\n",
    "            batch_idx += 1\n",
    "\n",
    "        eval_mse, eval_mape = evaluation(val_loader, model, model_name, LUR, SELECTED_COLUMN, mask)\n",
    "        \n",
    "\n",
    "        if eval_mse < best_mse:\n",
    "            best_model = deepcopy(model)\n",
    "            best_mse = eval_mse\n",
    "            mape = eval_mape\n",
    "            torch.save(best_model.state_dict(), \"./DeepLearning/save/\"+SELECTED_COLUMN+\"/\"+model_name+\".pth\")\n",
    "      \n",
    "    print(model_name)   \n",
    "    print(\"Best MSE :\", best_mse)\n",
    "    print(\"RMSE :\", np.sqrt(best_mse))\n",
    "    print(\"MAPE :\", mape)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearning.models import *\n",
    "from DeepLearning.loss_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set SoftDTW loss\n",
      "Start RNN training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.8993063675394121: 100%|██████████| 1001/1001 [01:04<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  18.453145430177823\n",
      "Eval RMSE: no2_median:  4.295712447333716\n",
      "Eval MAPE: no2_median:  52.77879786790665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.6160322225221866: 100%|██████████| 1001/1001 [01:03<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  21.327851235617153\n",
      "Eval RMSE: no2_median:  4.618208660900582\n",
      "Eval MAPE: no2_median:  52.23604465628269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.5380218581972894: 100%|██████████| 1001/1001 [01:04<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  24.670761555308577\n",
      "Eval RMSE: no2_median:  4.9669670378721635\n",
      "Eval MAPE: no2_median:  52.993001897963524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.48331043200491075: 100%|██████████| 1001/1001 [01:04<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  28.977265298117153\n",
      "Eval RMSE: no2_median:  5.383053529189279\n",
      "Eval MAPE: no2_median:  58.56292596920763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.4759328770604765: 100%|██████████| 1001/1001 [01:04<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  30.204099519482217\n",
      "Eval RMSE: no2_median:  5.495825644931089\n",
      "Eval MAPE: no2_median:  59.22082238616305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.5100386816632383: 100%|██████████| 1001/1001 [01:04<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  27.686106661872383\n",
      "Eval RMSE: no2_median:  5.26175889431209\n",
      "Eval MAPE: no2_median:  54.83562517365651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.47147798462288004: 100%|██████████| 1001/1001 [01:05<00:00, 15.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  27.32391434035042\n",
      "Eval RMSE: no2_median:  5.227228169914761\n",
      "Eval MAPE: no2_median:  53.56842424081459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.4299816055917418: 100%|██████████| 1001/1001 [01:03<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  25.355910041841003\n",
      "Eval RMSE: no2_median:  5.035465225958869\n",
      "Eval MAPE: no2_median:  52.95682132992285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.42727478792319634: 100%|██████████| 1001/1001 [01:04<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  26.29913866370293\n",
      "Eval RMSE: no2_median:  5.1282685834210096\n",
      "Eval MAPE: no2_median:  54.37685276175144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.41858842193737966: 100%|██████████| 1001/1001 [01:03<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  27.27022996208159\n",
      "Eval RMSE: no2_median:  5.222090573906353\n",
      "Eval MAPE: no2_median:  53.13114501442371\n",
      "RNN\n",
      "Best MSE : 18.453145430177823\n",
      "RMSE : 4.295712447333716\n",
      "MAPE : 52.77879786790665\n",
      "\n",
      "set SoftDTW loss\n",
      "Start LSTM training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.062140449126078: 100%|██████████| 1001/1001 [01:03<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  18.04005540664226\n",
      "Eval RMSE: no2_median:  4.247358638806271\n",
      "Eval MAPE: no2_median:  47.21234054246209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.6179881690220761: 100%|██████████| 1001/1001 [01:04<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  20.758885084335773\n",
      "Eval RMSE: no2_median:  4.556191949900242\n",
      "Eval MAPE: no2_median:  48.60122552975451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.5180357904857261: 100%|██████████| 1001/1001 [01:03<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  22.635762617677823\n",
      "Eval RMSE: no2_median:  4.757705604351516\n",
      "Eval MAPE: no2_median:  50.00663980779289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.526094950575541: 100%|██████████| 1001/1001 [01:03<00:00, 15.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  19.951151444822177\n",
      "Eval RMSE: no2_median:  4.466671181632042\n",
      "Eval MAPE: no2_median:  52.17643961248039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.437750859102318: 100%|██████████| 1001/1001 [01:03<00:00, 15.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  23.925660711950837\n",
      "Eval RMSE: no2_median:  4.891386379335703\n",
      "Eval MAPE: no2_median:  46.91514529942469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.4682145369844837: 100%|██████████| 1001/1001 [01:03<00:00, 15.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 1912.0\n",
      "Eval MSE:  25.778516442207113\n",
      "Eval RMSE: no2_median:  5.077254813598301\n",
      "Eval MAPE: no2_median:  53.54466458244802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.41314373907822: 100%|██████████| 1001/1001 [01:04<00:00, 15.62it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# LSTM\u001b[39;00m\n\u001b[1;32m     10\u001b[0m LSTMmodel \u001b[39m=\u001b[39m LSTM(\u001b[39m1\u001b[39m, INPUT_DIM\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, HIDDEN_DIM, LAYER_DIM)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m train(sampleLoader, val_loader, LSTMmodel, \u001b[39m\"\u001b[39;49m\u001b[39mLSTM\u001b[39;49m\u001b[39m\"\u001b[39;49m, SELECTED_COLUMN)\n",
      "Cell \u001b[0;32mIn[11], line 142\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(sampleLoader, val_loader, model, model_name, SELECTED_COLUMN, mask, LUR, l1, l2)\u001b[0m\n\u001b[1;32m    139\u001b[0m     bar\u001b[39m.\u001b[39mset_description(\u001b[39mstr\u001b[39m(epoch_loss))\n\u001b[1;32m    140\u001b[0m     batch_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 142\u001b[0m eval_mse, eval_mape \u001b[39m=\u001b[39m evaluation(val_loader, model, model_name, LUR, SELECTED_COLUMN, mask)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m eval_mse \u001b[39m<\u001b[39m best_mse:\n\u001b[1;32m    146\u001b[0m     best_model \u001b[39m=\u001b[39m deepcopy(model)\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(val_loader, model, model_name, LUR, SELECTED_COLUMN, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m total_pe \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     16\u001b[0m total_valid \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfor\u001b[39;00m x_val, _, y_val \u001b[39min\u001b[39;00m val_loader:\n\u001b[1;32m     19\u001b[0m     x_val, y_val \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39mfloat() \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m (x_val, y_val)]\n\u001b[1;32m     21\u001b[0m     \u001b[39mif\u001b[39;00m mask:\n",
      "File \u001b[0;32m/home/iknow/anaconda3/envs/ma/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/iknow/anaconda3/envs/ma/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/home/iknow/anaconda3/envs/ma/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1324\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1326\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1327\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/home/iknow/anaconda3/envs/ma/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1151\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1164\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1165\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/home/iknow/anaconda3/envs/ma/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/home/iknow/anaconda3/envs/ma/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/home/iknow/anaconda3/envs/ma/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/home/iknow/anaconda3/envs/ma/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/home/iknow/anaconda3/envs/ma/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SELECTED_COLUMN = \"no2_median\" # [\"pm25_median\", \"so2_median\", \"pm10_median\", \"no2_median\", \"o3_median\", \"co_median\"]:\n",
    "train_data = CityDataP(SELECTED_COLUMN, \"train\")\n",
    "val_data = CityDataP(SELECTED_COLUMN, \"test\")\n",
    "sampleLoader = DataLoader(train_data, 32, shuffle=True, num_workers=4, worker_init_fn=seed_worker, generator=g)\n",
    "val_loader = DataLoader(val_data, 4096, shuffle=False, num_workers=4, worker_init_fn=seed_worker, generator=g)\n",
    "# RNN\n",
    "RNNmodel = RNN(1, INPUT_DIM+1, HIDDEN_DIM, LAYER_DIM).to(device)\n",
    "train(sampleLoader, val_loader, RNNmodel, \"RNN\", SELECTED_COLUMN)\n",
    "# LSTM\n",
    "LSTMmodel = LSTM(1, INPUT_DIM+1, HIDDEN_DIM, LAYER_DIM).to(device)\n",
    "train(sampleLoader, val_loader, LSTMmodel, \"LSTM\", SELECTED_COLUMN)\n",
    "# # BiLSTM\n",
    "# BiLSTMmodel = LSTM(1, INPUT_DIM+1, HIDDEN_DIM, LAYER_DIM, bidirectional=True).to(device)\n",
    "# train(sampleLoader, val_loader, BiLSTMmodel, \"BiLSTM\")\n",
    "# # TransLSTM\n",
    "# TransLSTMmodel = TransLSTM(num_layers=3, D=16, H=5, hidden_mlp_dim=32, inp_features=11, out_features=1, dropout_rate=0.2, LSTM_module = LSTM(4, INPUT_DIM+1, HIDDEN_DIM, LAYER_DIM, bidirectional = False).to(device), attention_type='regular').to(device) # cosine_square, cosine, regular # 6L, 12H\n",
    "# train(sampleLoader, val_loader, TransLSTMmodel, \"TransLSTM\", SELECTED_COLUMN, mask=True)\n",
    "# # Transformer\n",
    "# Transmodel = Transformer(num_layers=6, D=16, H=10, hidden_mlp_dim=32, inp_features=11, out_features=1, dropout_rate=0.1, attention_type='regular', SL=SEQ_LENGTH).to(device) # cosine_square, cosine, regular # 6L, 12H\n",
    "# train(sampleLoader, val_loader, Transmodel, \"Transformer\", SELECTED_COLUMN, mask=True)\n",
    "# # CosFormer\n",
    "# TransCosModel = Transformer(num_layers=6, D=16, H=10, hidden_mlp_dim=32, inp_features=11, out_features=1, dropout_rate=0.1, attention_type='cosine', SL=SEQ_LENGTH).to(device) # cosine_square, cosine, regular # 6L, 12\n",
    "# train(sampleLoader, val_loader, TransCosModel, \"CosFormer\", SELECTED_COLUMN, mask=True)\n",
    "# # CosSquareFormer\n",
    "# TransCosSquare = Transformer(num_layers=6, D=16, H=10, hidden_mlp_dim=32, inp_features=11, out_features=1, dropout_rate=0.1, attention_type='cosine_square', SL=SEQ_LENGTH).to(device) # cosine_square, cosine, regular # 6L, 12H\n",
    "# train(sampleLoader, val_loader, TransCosSquare, \"CosSquareFormer\", SELECTED_COLUMN, mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransLSTM\n",
    "TransLSTMmodel = TransLSTM(num_layers=3, D=16, H=5, hidden_mlp_dim=32, inp_features=11, out_features=1, dropout_rate=0.2, LSTM_module = LSTM(4, INPUT_DIM+1, HIDDEN_DIM, LAYER_DIM, bidirectional = False).to(device), attention_type='regular').to(device) # cosine_square, cosine, regular # 6L, 12H\n",
    "train(sampleLoader, val_loader, TransLSTMmodel, \"TransLSTM\", SELECTED_COLUMN, mask=True)\n",
    "# Transformer\n",
    "Transmodel = Transformer(num_layers=6, D=16, H=10, hidden_mlp_dim=32, inp_features=11, out_features=1, dropout_rate=0.1, attention_type='regular', SL=SEQ_LENGTH).to(device) # cosine_square, cosine, regular # 6L, 12H\n",
    "train(sampleLoader, val_loader, Transmodel, \"Transformer\", SELECTED_COLUMN, mask=True)\n",
    "# CosFormer\n",
    "TransCosModel = Transformer(num_layers=6, D=16, H=10, hidden_mlp_dim=32, inp_features=11, out_features=1, dropout_rate=0.1, attention_type='cosine', SL=SEQ_LENGTH).to(device) # cosine_square, cosine, regular # 6L, 12\n",
    "train(sampleLoader, val_loader, TransCosModel, \"CosFormer\", SELECTED_COLUMN, mask=True)\n",
    "# CosSquareFormer\n",
    "TransCosSquare = Transformer(num_layers=6, D=16, H=10, hidden_mlp_dim=32, inp_features=11, out_features=1, dropout_rate=0.1, attention_type='cosine_square', SL=SEQ_LENGTH).to(device) # cosine_square, cosine, regular # 6L, 12H\n",
    "train(sampleLoader, val_loader, TransCosSquare, \"CosSquareFormer\", SELECTED_COLUMN, mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid samples: 2769.0\n",
      "Eval MSE:  1264.4826652221018\n",
      "Eval RMSE: o3_median:  35.55956503139629\n",
      "Eval MAPE: o3_median:  187.16562697499097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1264.4826652221018, 187.16562697499097)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SELECTED_COLUMN = \"so2_median\" # [\"pm25_median\", \"so2_median\", \"pm10_median\", \"no2_median\", \"o3_median\", \"co_median\"]:\n",
    "train_data = CityDataP(SELECTED_COLUMN, \"train\")\n",
    "val_data = CityDataP(SELECTED_COLUMN, \"test\")\n",
    "\n",
    "sampleLoader = DataLoader(train_data, 32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, 4096, shuffle=False, num_workers=4)\n",
    "\n",
    "OLS = LinearRegression(input_dim=11)\n",
    "OLS.load_state_dict(torch.load(\"./DeepLearning/save/\"+SELECTED_COLUMN+\"/OLS.pth\"))\n",
    "evaluation(val_loader, OLS, \"OLS\")\n",
    "\n",
    "\n",
    "SELECTED_COLUMN = \"no2_median\" # [\"pm25_median\", \"so2_median\", \"pm10_median\", \"no2_median\", \"o3_median\", \"co_median\"]:\n",
    "train_data = CityDataP(SELECTED_COLUMN, \"train\")\n",
    "val_data = CityDataP(SELECTED_COLUMN, \"test\")\n",
    "\n",
    "sampleLoader = DataLoader(train_data, 32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, 4096, shuffle=False, num_workers=4)\n",
    "\n",
    "OLS = LinearRegression(input_dim=11)\n",
    "OLS.load_state_dict(torch.load(\"./DeepLearning/save/\"+SELECTED_COLUMN+\"/OLS.pth\"))\n",
    "evaluation(val_loader, OLS, \"OLS\")\n",
    "\n",
    "\n",
    "SELECTED_COLUMN = \"o3_median\" # [\"pm25_median\", \"so2_median\", \"pm10_median\", \"no2_median\", \"o3_median\", \"co_median\"]:\n",
    "train_data = CityDataP(SELECTED_COLUMN, \"train\")\n",
    "val_data = CityDataP(SELECTED_COLUMN, \"test\")\n",
    "\n",
    "sampleLoader = DataLoader(train_data, 32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, 4096, shuffle=False, num_workers=4)\n",
    "\n",
    "OLS = LinearRegression(input_dim=11)\n",
    "OLS.load_state_dict(torch.load(\"./DeepLearning/save/\"+SELECTED_COLUMN+\"/OLS.pth\"))\n",
    "evaluation(val_loader, OLS, \"OLS\")\n",
    "\n",
    "\n",
    "SELECTED_COLUMN = \"co_median\" # [\"pm25_median\", \"so2_median\", \"pm10_median\", \"no2_median\", \"o3_median\", \"co_median\"]:\n",
    "train_data = CityDataP(SELECTED_COLUMN, \"train\")\n",
    "val_data = CityDataP(SELECTED_COLUMN, \"test\")\n",
    "\n",
    "sampleLoader = DataLoader(train_data, 32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, 4096, shuffle=False, num_workers=4)\n",
    "\n",
    "OLS = LinearRegression(input_dim=11)\n",
    "OLS.load_state_dict(torch.load(\"./DeepLearning/save/\"+SELECTED_COLUMN+\"/OLS.pth\"))\n",
    "evaluation(val_loader, OLS, \"OLS\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
